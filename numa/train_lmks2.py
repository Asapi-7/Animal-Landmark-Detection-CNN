# =================================================================
# 0. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# =================================================================
import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image, ImageDraw, ImageFont 
import glob
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt 
from tqdm import tqdm
import numpy as np 

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚µã‚¤ã‚º (ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚³ãƒ¼ãƒ‰ã¨åˆã‚ã›ã‚‹)
IMG_SIZE = 224
NUM_LANDMARKS = 9

# =================================================================
# 1. ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯åº§æ¨™ (.pts) ã®èª­ã¿è¾¼ã¿é–¢æ•°
# =================================================================
def load_landmarks_from_pts_to_tensor(pts_path):
    """ .ptsãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰9ç‚¹ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯åº§æ¨™ã‚’èª­ã¿è¾¼ã¿ã€å¹³å¦åŒ–ã•ã‚ŒãŸTensor [18] ã§è¿”ã™ """
    points = []
    with open(pts_path, 'r') as f:
        lines = f.readlines()

    start_index = -1
    for i, line in enumerate(lines):
        if line.strip() == '{':
            start_index = i + 1
            break
            
    # 9ç‚¹åˆ†ã‚’æŠ½å‡º
    for line in lines[start_index : start_index + 9]:
        try:
            x, y = map(float, line.strip().split())
            points.extend([x, y]) # [x1, y1, x2, y2, ...] ã®é † (18è¦ç´ )
        except ValueError:
             # ä¸æ­£ãªè¡Œã¯ç„¡è¦–
             continue

    if len(points) != 18:
        raise ValueError(f"Expected 18 coordinates (9 points), but found {len(points)} in {pts_path}")

    return torch.tensor(points, dtype=torch.float32)

# =================================================================
# 2. PyTorch Dataset ã‚¯ãƒ©ã‚¹
# =================================================================
class LandmarkDataset(Dataset):
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‚’å¤–éƒ¨ã‹ã‚‰å—ã‘å–ã‚‹ã‚ˆã†ä¿®æ­£ (train/teståˆ†å‰²ã«å¿…è¦)
    def __init__(self, file_paths):
        self.image_files = file_paths

        # ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ã«åˆã‚ã›ãŸæœ€çµ‚çš„ãªç”»åƒå¤‰æ› (æ­£è¦åŒ–)
        self.transform = transforms.Compose([
            transforms.ToTensor(), # HWC -> CHW, 0-255 -> 0-1
            # ImageNetã®çµ±è¨ˆå€¤ã§æ¨™æº–åŒ–
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        
        # .pts ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ .jpg ãƒ‘ã‚¹ã‹ã‚‰æ§‹ç¯‰
        pts_path = img_path.replace(".jpg", ".pts") 

        # è¨“ç·´ç”¨ç”»åƒ (æ­£è¦åŒ–æ¸ˆã¿)
        image = Image.open(img_path).convert("RGB") # ç”»åƒã‚’RGBã§èª­ã¿è¾¼ã¿
        transformed_image = self.transform(image) # å¤‰æ›ã¨æ­£è¦åŒ–ã‚’å®Ÿè¡Œ

        landmarks = load_landmarks_from_pts_to_tensor(pts_path) # åº§æ¨™ [18] ã‚’èª­ã¿è¾¼ã¿

        # æ¨è«–ãƒ»æç”»ã®ãŸã‚ã«å…ƒã®ç”»åƒãƒ‘ã‚¹ã‚‚è¿”ã™
        return transformed_image, landmarks, img_path # <--- å¤‰æ›´: img_path ã‚’è¿½åŠ 

# =================================================================
# 3. ãƒ¢ãƒ‡ãƒ«å®šç¾© (ã‚«ã‚¹ã‚¿ãƒ ResNet18ã‚’ä½¿ç”¨)
# =================================================================

class BasicBlock(nn.Module):
    '''
    ResNet18ã«ãŠã‘ã‚‹æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯
    '''
    def __init__(self, in_channels: int, out_channels: int,
                 stride: int=1):
        super().__init__()

        # æ®‹å·®æ¥ç¶š
        self.conv1 = nn.Conv2d(in_channels, out_channels,
                               kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels,
                               kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

        # ã‚¹ã‚­ãƒƒãƒ—æ¥ç¶šã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (å¯¸æ³•åˆã‚ã›)
        self.downsample = None
        if stride != 1 or in_channels != out_channels:
             self.downsample = nn.Sequential(
                 nn.Conv2d(in_channels, out_channels, kernel_size=1,
                           stride=stride, bias=False),
                 nn.BatchNorm2d(out_channels)
             )

    def forward(self, x: torch.Tensor):
        identity = x # æ’ç­‰å†™åƒ (ã‚¹ã‚­ãƒƒãƒ—æ¥ç¶š) ã‚’ä¿å­˜

        # æ®‹å·®å†™åƒ
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)

        # ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‡¦ç†
        if self.downsample is not None:
             identity = self.downsample(identity)

        # æ®‹å·®å†™åƒã¨æ’ç­‰å†™åƒã®è¦ç´ æ¯ã®å’Œã‚’è¨ˆç®—
        out += identity

        out = self.relu(out)

        return out

class ResNet18(nn.Module):
    '''
    ResNet18ãƒ¢ãƒ‡ãƒ«
    num_classes: åˆ†é¡å¯¾è±¡ã®ç‰©ä½“ã‚¯ãƒ©ã‚¹æ•° (ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯å›å¸°ç”¨ã«ç½®ãæ›ãˆã‚‰ã‚Œã‚‹)
    '''
    def __init__(self, num_classes: int):
        super().__init__()

        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2,
                               padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)

        self.max_pool = nn.MaxPool2d(kernel_size=3,
                                     stride=2, padding=1)

        self.layer1 = nn.Sequential(
            BasicBlock(64, 64),
            BasicBlock(64, 64),
        )
        self.layer2 = nn.Sequential(
            BasicBlock(64, 128, stride=2),
            BasicBlock(128, 128),
        )
        self.layer3 = nn.Sequential(
            BasicBlock(128, 256, stride=2),
            BasicBlock(256, 256),
        )
        self.layer4 = nn.Sequential(
            BasicBlock(256, 512, stride=2),
            BasicBlock(512, 512),
        )

        self.avg_pool = nn.AdaptiveAvgPool2d(1)

        # å›å¸°ãƒ˜ãƒƒãƒ‰ã®å¤šå±¤åŒ–ã«å‚™ãˆã¦ã€æš«å®šçš„ãªç·šå½¢å±¤ã‚’å®šç¾©
        self.linear = nn.Linear(512, num_classes) 

    def forward(self, x: torch.Tensor, return_embed: bool=False):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.max_pool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avg_pool(x)
        x = x.flatten(1)

        if return_embed:
            return x

        # LandmarkRegressorã§ç½®ãæ›ãˆã‚‰ã‚Œã‚‹ç·šå½¢å±¤
        x = self.linear(x) 

        return x

class LandmarkRegressor(nn.Module):
    def __init__(self, num_landmarks=9):
        super(LandmarkRegressor, self).__init__()
        
        # 1. Backbone: ã‚«ã‚¹ã‚¿ãƒ ResNet18ã‚’ä½¿ç”¨
        self.backbone = ResNet18(num_classes=1000)
        
        # 2. Head: Denseå±¤ (æœ€çµ‚å±¤) ã®å¤‰æ›´
        num_features = self.backbone.linear.in_features  # 512
        output_dim = num_landmarks * 2                   # 9 * 2 = 18

        # --- ğŸ’¥ å›å¸°ãƒ˜ãƒƒãƒ‰ã®å¼·åŒ– ğŸ’¥ ---
        HIDDEN_DIM = 256
        DROPOUT_RATE = 0.5
        
        # å¤šå±¤å›å¸°ãƒ˜ãƒƒãƒ‰ã®å®šç¾© (512 -> 256 -> 18)
        self.regressor_head = nn.Sequential(
            nn.Linear(num_features, HIDDEN_DIM),
            nn.ReLU(inplace=True),
            nn.Dropout(p=DROPOUT_RATE),
            nn.Linear(HIDDEN_DIM, output_dim)
        )
        
        # 3. ResNet18ã®ç·šå½¢å±¤ã‚’ã€å¼·åŒ–ã—ãŸå›å¸°ãƒ˜ãƒƒãƒ‰ã«ç½®ãæ›ãˆ
        self.backbone.linear = self.regressor_head

    def forward(self, x):
        return self.backbone(x)

# =================================================================
# 4. è©•ä¾¡é–¢æ•° (evaluate_model)
# =================================================================
def evaluate_model(model, data_loader, criterion, device):
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹æå¤±ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’ç¢ºèªã™ã‚‹"""
    model.eval()
    total_loss = 0
    total_nme = 0
    count = 0

    with torch.no_grad():
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ imgs, labels ã®ã¿ã«å¤‰æ›´ (img_path ã¯è©•ä¾¡ã«ä½¿ã‚ãªã„ãŸã‚)
        for data in data_loader:
            imgs, labels = data[0].to(device), data[1].to(device)
            outputs = model(imgs)

            loss = criterion(outputs, labels)
            total_loss += loss.item() * imgs.size(0)

            # --- NMEã®è¨ˆç®—ã¨é›†è¨ˆ ---
            nme_batch = calculate_nme(outputs, labels, device)
            total_nme += nme_batch.item() * imgs.size(0)

            count += imgs.size(0)

    avg_loss = total_loss / count
    avg_nme = total_nme / count
    return avg_loss, avg_nme

# =================================================================
# 4.1. NME (Normalized Mean Error) è¨ˆç®—é–¢æ•°
# =================================================================
def calculate_normalization_factor(landmarks):
    """ 
    ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ [N, 18] ã‹ã‚‰ã€ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®å¯¾è§’ç·šé•·ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    """
    # åº§æ¨™ã‚’ (N, 9, 2) ã«æ•´å½¢: (x1, y1, x2, y2, ...) -> ((x1, y1), (x2, y2), ...)
    coords = landmarks.reshape(-1, 9, 2)
    
    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®è¨ˆç®— (å…¨ç‚¹ã® min/max ã‚’ä½¿ç”¨)
    x_min = coords[..., 0].min(dim=1).values
    x_max = coords[..., 0].max(dim=1).values
    y_min = coords[..., 1].min(dim=1).values
    y_max = coords[..., 1].max(dim=1).values
    
    # å¯¾è§’ç·šé•·ã®è¨ˆç®—: sqrt((x_max - x_min)^2 + (y_max - y_min)^2)
    width = x_max - x_min
    height = y_max - y_min
    
    # å¯¾è§’ç·šé•· (NMEã®æ­£è¦åŒ–åŸºæº–)
    diagonal = torch.sqrt(width**2 + height**2)
    
    # å¯¾è§’ç·šé•·ãŒã‚¼ãƒ­ã«ãªã‚‹ã®ã‚’é˜²ããŸã‚ã€å°ã•ãªå€¤ã‚’åŠ ãˆã‚‹
    return diagonal + 1e-6


def calculate_nme(outputs, labels, device):
    """ NME (Normalized Mean Error) ã‚’è¨ˆç®—ã™ã‚‹ """
    num_landmarks = 9
    
    # å‡ºåŠ›ã¨æ­£è§£ã‚’ (N, 9, 2) ã«æ•´å½¢
    outputs_reshaped = outputs.reshape(-1, num_landmarks, 2)
    labels_reshaped = labels.reshape(-1, num_landmarks, 2)

    # 1. äºˆæ¸¬åº§æ¨™ã¨æ­£è§£åº§æ¨™é–“ã®ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’è¨ˆç®— (å„ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã”ã¨)
    distances = torch.linalg.norm(outputs_reshaped - labels_reshaped, dim=2) # [N, 9]

    # 2. æ­£è¦åŒ–ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®å¯¾è§’ç·šé•·ï¼‰ã‚’è¨ˆç®—
    normalization_factors = calculate_normalization_factor(labels).to(device) # [N]

    # 3. å„ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã®è·é›¢ã‚’æ­£è¦åŒ–ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã§å‰²ã‚‹
    # unsqueeze(1) ã§ [N] -> [N, 1] ã«ã—ã¦ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚’å¯èƒ½ã«ã™ã‚‹
    normalized_distances = distances / normalization_factors.unsqueeze(1) # [N, 9]

    # 4. å…¨ã¦ã®æ­£è¦åŒ–è·é›¢ã®å¹³å‡ã‚’å–ã‚‹ (ã“ã‚ŒãŒ NME)
    nme = normalized_distances.mean()

    return nme

# =================================================================
# 5. æå¤±æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆé–¢æ•°
# =================================================================
def plot_loss_curve(train_losses, test_losses, num_epochs):
    """è¨“ç·´æå¤±ã¨ãƒ†ã‚¹ãƒˆæå¤±ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹"""
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss (MSE)', marker='o')
    if test_losses:
        plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss (MSE)', marker='s')
    
    plt.title('Training and Test Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Mean Squared Error (MSE) Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

# =================================================================
# 6. ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æç”»ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (PILç”¨ã€ç¾åœ¨ã¯æœªä½¿ç”¨)
# =================================================================
def draw_landmarks_pil(image, landmarks, color='red', point_size=5):
    """
    PIL Image ã«ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚’æç”»ã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (1-9) ã‚’ä»˜ä¸ã™ã‚‹
    """
    draw = ImageDraw.Draw(image)
    
    try:
        font_size = 18
        # ã‚·ã‚¹ãƒ†ãƒ ã« arial.ttf ãŒå­˜åœ¨ã—ãªã„å ´åˆãŒã‚ã‚‹ãŸã‚æ³¨æ„
        font = ImageFont.truetype("arial.ttf", size=font_size) 
    except IOError:
        font = ImageFont.load_default() 
    
    if isinstance(landmarks, torch.Tensor):
        landmarks = landmarks.cpu().numpy()
        
    for k in range(0, len(landmarks) // 2):
        i = k * 2
        x = int(landmarks[i])
        y = int(landmarks[i+1])
        
        landmark_index = k + 1 
        
        bbox = [x - point_size, y - point_size, x + point_size, y + point_size]
        draw.ellipse(bbox, fill=color)
        
        text_position = (x + point_size + 2, y + point_size + 2) 
        draw.text(text_position, str(landmark_index), fill="yellow", font=font)
        
    return image

# =================================================================
# 7. äºˆæ¸¬çµæœã‚’ç”»åƒã«æç”»ã—ã¦ä¿å­˜ã™ã‚‹é–¢æ•° 
# =================================================================
def save_landmark_predictions(model, data_loader, device, num_samples=5, save_dir="./predictions_output"):
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æ¨è«–ã‚’è¡Œã„ã€äºˆæ¸¬ã•ã‚ŒãŸãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚’ç”»åƒã«æç”»ã—ã¦ä¿å­˜ã™ã‚‹
    """
    model.eval()
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    saved_count = 0
    
    with torch.no_grad():
        for images, targets, img_paths in data_loader:
            if saved_count >= num_samples:
                break
                
            images_tensor = images.to(device)
            outputs = model(images_tensor).cpu() # æ¨è«–çµæœã‚’CPUã«æˆ»ã™

            for i in range(images_tensor.size(0)):
                if saved_count >= num_samples:
                    break
                    
                # 1. å…ƒã®ç”»åƒã®èª­ã¿è¾¼ã¿
                original_img_path = img_paths[i]
                original_image_pil = Image.open(original_img_path).convert("RGB")
                
                # 2. äºˆæ¸¬ã•ã‚ŒãŸãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯åº§æ¨™ã‚’å–å¾— (NumPyé…åˆ—)
                predicted_landmarks_flat = outputs[i].numpy() # [x1,y1,x2,y2,...]
                predicted_landmarks_reshaped = predicted_landmarks_flat.reshape(-1, 2) # (9, 2)

                # --- Matplotlibã§æç”» ---
                fig, ax = plt.subplots(figsize=(8, 8))
                ax.imshow(original_image_pil)
                
                # äºˆæ¸¬åº§æ¨™ã‚’å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                original_w, original_h = original_image_pil.size
                
                # IMG_SIZE ã¯ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã‚µã‚¤ã‚ºï¼ˆ224ï¼‰
                scaled_landmarks_x = predicted_landmarks_reshaped[:, 0] * (original_w / IMG_SIZE)
                scaled_landmarks_y = predicted_landmarks_reshaped[:, 1] * (original_h / IMG_SIZE)
                
                scaled_landmarks = np.stack([scaled_landmarks_x, scaled_landmarks_y], axis=1) # (9, 2) ã®å½¢å¼ã«å†æ§‹æˆ

                # --- A. 1, 2å€‹ç›®ã®ç‚¹ã‚’ç›´å¾„ã¨ã™ã‚‹å†† (èµ¤è‰²) ã®æç”» ---
                p1 = scaled_landmarks[0]
                p2 = scaled_landmarks[1]
                center_x12 = (p1[0] + p2[0]) / 2
                center_y12 = (p1[1] + p2[1]) / 2
                diameter12 = np.linalg.norm(p1 - p2)
                radius12 = diameter12 / 2
                circle12 = plt.Circle((center_x12, center_y12), radius12, 
                                     color='red', fill=False, linewidth=2)
                ax.add_artist(circle12)
                
                # --- B. 3, 4å€‹ç›®ã®ç‚¹ã‚’ç›´å¾„ã¨ã™ã‚‹å†† (èµ¤è‰²) ã®æç”» ---
                p3 = scaled_landmarks[2]
                p4 = scaled_landmarks[3]
                center_x34 = (p3[0] + p4[0]) / 2
                center_y34 = (p3[1] + p4[1]) / 2
                diameter34 = np.linalg.norm(p3 - p4)
                radius34 = diameter34 / 2
                circle34 = plt.Circle((center_x34, center_y34), radius34, 
                                     color='red', fill=False, linewidth=2)
                ax.add_artist(circle34)

                # --- C. 6, 8, 7, 9, 6ã®é †ã«ç›´ç·šã‚’ã¤ãªã’ãŸç·š (èµ¤è‰²) ã®æç”» ---
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ 0 ã‹ã‚‰å§‹ã¾ã‚‹ãŸã‚ã€(6, 8, 7, 9, 6) -> [5, 7, 6, 8, 5]
                indices = [5, 7, 6, 8, 5] 
                line_x = scaled_landmarks_x[indices]
                line_y = scaled_landmarks_y[indices]
                ax.plot(line_x, line_y, color='red', linestyle='-', linewidth=2)

                # --- ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ç‚¹ã‚’æç”» ---
                ax.scatter(scaled_landmarks_x, scaled_landmarks_y, 
                           c='red', marker='o', s=50, label=None)
                
                # --- ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã«ç•ªå·ã‚’æŒ¯ã‚‹ ---
                # æ³¨é‡ˆæç”»ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯ç°¡ç•¥åŒ–ã—ã¾ã—ãŸ (æ–‡å­—ã®å½±ä»˜ã‘ã‚’çœç•¥)
                tmp = 10
                for k_idx in range(NUM_LANDMARKS): 
                     ax.annotate(str(k_idx+1), (scaled_landmarks_x[k_idx] + tmp + 2, scaled_landmarks_y[k_idx] + tmp + 2), color='yellow', fontsize=20, ha='center', va='center',
                                 path_effects=[plt.matplotlib.patheffects.withStroke(linewidth=3, foreground='black')])


                ax.set_title(f"Predicted Landmarks (Sample {saved_count+1})")
                ax.axis('off') # è»¸ã‚’éè¡¨ç¤ºã«
                
                # --- ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ ---
                base_name = os.path.basename(original_img_path)
                save_path = os.path.join(save_dir, f"pred_geometric_{base_name}")
                plt.savefig(save_path, bbox_inches='tight', pad_inches=0) # ä½™ç™½ãªã—ã§ä¿å­˜
                plt.close(fig) # ç¾åœ¨ã®å›³ã‚’é–‰ã˜ã¦ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
                
                print(f"âœ… äºˆæ¸¬ç”»åƒã‚’ä¿å­˜: {save_path}")
                saved_count += 1

# =================================================================
# 8. ãƒ¡ã‚¤ãƒ³ã®è¨“ç·´é–¢æ•° (train/teståˆ†å‰²ã‚’å«ã‚€)
# =================================================================
def train_model():
    # --- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š ---
    DATA_DIR = "./cropped_dataset" # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹
    TEST_SIZE = 0.2 # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ (20%)
    BATCH_SIZE = 64 # 11/6 1: 32ã¨ã™ã‚‹
    NUM_LANDMARKS = 9
    NUM_EPOCHS = 32 # 11/6 1: 20ã¨ã™ã‚‹
    LEARNING_RATE = 0.001
    
    # --- ãƒ‡ãƒã‚¤ã‚¹è¨­å®š ---
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"è¨“ç·´ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # --- ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã®å–å¾—ã¨åˆ†å‰² (train/teståˆ†å‰²) ---
    try:
        all_files = glob.glob(os.path.join(DATA_DIR, "*.jpg"))
        if not all_files:
            raise FileNotFoundError("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã«.jpgãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        
        # train/teståˆ†å‰²
        train_files, test_files = train_test_split(
            all_files, test_size=TEST_SIZE, random_state=42
        )
        print(f"å…¨ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_files)}")
        print(f"Train: {len(train_files)} files, Test: {len(test_files)} files")
        
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
        print("cropped_dataset ãƒ•ã‚©ãƒ«ãƒ€ãŒ./ (ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª) ã«å­˜åœ¨ã—ã€ãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
    
    # --- ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™ ---
    train_dataset = LandmarkDataset(train_files)
    test_dataset = LandmarkDataset(test_files)
        
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4
    )
    # ãƒ†ã‚¹ãƒˆæ™‚ã®ç”»åƒãƒ‘ã‚¹ã®å¯¾å¿œã®ãŸã‚ shuffle=False ã«ã—ã¦ãŠã
    test_loader = DataLoader(
        test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4 
    )
    
    # --- ãƒ¢ãƒ‡ãƒ«ã€æå¤±é–¢æ•°ã€æœ€é©åŒ–æ‰‹æ³•ã®è¨­å®š ---
    model = LandmarkRegressor(num_landmarks=NUM_LANDMARKS)
    
    # ã‚¹ã‚¯ãƒ©ãƒƒãƒå­¦ç¿’ã®ãŸã‚ã€é‡ã¿ã¯ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã®ã¾ã¾
    # è»¢ç§»å­¦ç¿’ã‚’é¿ã‘ã‚‹ãŸã‚ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã¯ãƒ­ãƒ¼ãƒ‰ã—ãªã„
    print("\nãƒ¢ãƒ‡ãƒ«ã¯ãƒ©ãƒ³ãƒ€ãƒ ãªé‡ã¿ã§åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ (è»¢ç§»å­¦ç¿’å›é¿)ã€‚")

    model.to(device)
    
    criterion = nn.MSELoss() 
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # --- æå¤±è¨˜éŒ²ç”¨ã®ãƒªã‚¹ãƒˆ ---
    train_losses = []
    test_losses = []
    
    # --- è¨“ç·´ãƒ«ãƒ¼ãƒ— ---
    print("\n--- è¨“ç·´é–‹å§‹ ---")
    
    for epoch in range(NUM_EPOCHS):
        model.train() # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰
        running_loss = 0.0
        
        # tqdm ã‚’ä½¿ç”¨ã—ã¦é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤º
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ images, targets, img_paths ã‚’å—ã‘å–ã‚‹
        for i, (images, targets, _) in enumerate(tqdm(train_loader, desc=f"Epoch [{epoch+1}/{NUM_EPOCHS}]")):
            images = images.to(device)
            targets = targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            
        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # --- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡ ---
        test_loss, test_nme = evaluate_model(model, test_loader, criterion, device)
        test_losses.append(test_loss)
        
        print(f"--- Epoch [{epoch+1}/{NUM_EPOCHS}] å®Œäº†. Train Loss: {avg_train_loss:.4f}, Test Loss: {test_loss:.4f}, Test NME: {test_nme:.4f} ---")

    # --- æœ€çµ‚è©•ä¾¡ã¨ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ ---
    final_test_loss , final_test_nme = evaluate_model(model, test_loader, criterion, device)
    print(f"\nâœ… Final Test Loss: {final_test_loss:.4f}, Final Test NME: {final_test_nme:.4f}")

    MODEL_PATH_SAVE = 'landmark_regressor_scratch_improved.pth' # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹
    torch.save(model.state_dict(), MODEL_PATH_SAVE)
    print(f"ãƒ¢ãƒ‡ãƒ«ãŒ '{MODEL_PATH_SAVE}' ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
    
    # --- å­¦ç¿’æ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ ---
    print("\n--- å­¦ç¿’æ›²ç·šã‚’è¡¨ç¤º ---")
    plot_loss_curve(train_losses, test_losses, NUM_EPOCHS)
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ†ã‚¹ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’è¿”ã—ã¦ã€å¾Œç¶šã®å‡¦ç†ã§åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
    return model, test_loader, device 

# =================================================================
# 9. ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ 
# =================================================================
if __name__ == '__main__':
    # è¨“ç·´ã‚’å®Ÿè¡Œã—ã€è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ãƒ†ã‚¹ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—
    trained_model, test_loader, device = train_model()

    # äºˆæ¸¬çµæœã®æç”»ã¨ä¿å­˜ã‚’å®Ÿè¡Œ
    print("\n--- äºˆæ¸¬ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã®æç”»ã¨ä¿å­˜ã‚’é–‹å§‹ ---")
    save_landmark_predictions(
        model=trained_model, 
        data_loader=test_loader, 
        device=device, 
        num_samples=5, 
        save_dir="./predictions_output" # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    )
    print("--- äºˆæ¸¬ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã®æç”»ã¨ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸã€‚---")