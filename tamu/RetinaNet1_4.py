import os
import torch
import numpy as np
import time
from PIL import Image
from torch.utils.data import Dataset
from torchvision.transforms import functional as F
from torchvision import transforms as T
from torch.utils.data import DataLoader
import torchvision.transforms.v2 as T_v2
from torchvision.tv_tensors import BoundingBoxes, Mask, Image as TVImage
import glob # ğŸ‘ˆ è¿½åŠ : ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆå–å¾—ç”¨
from sklearn.model_selection import train_test_split # ğŸ‘ˆ è¿½åŠ : ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ç”¨

# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ç”¨
from resnet18_backbone import resnet18
from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor
from torchvision.ops.feature_pyramid_network import LastLevelP6P7 
from torchvision.models.detection.anchor_utils import AnchorGenerator
from torchvision.models.detection import RetinaNet
from tqdm import tqdm

import torch.optim as optim
from torchvision.ops import box_iou

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
class CustomObjectDetectionDataset(Dataset):
    # âš ï¸ __init__ã‚’ä¿®æ­£: rootã§ã¯ãªãã€ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹
    def __init__(self, img_list, root, transforms=None):
        self.root = root # .ptsãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«rootã‚’ä¿æŒ
        self.transforms = transforms
        self.imgs = img_list # ğŸ‘ˆ æ—¢ã«åˆ†å‰²ã•ã‚ŒãŸç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
        
    def _parse_pts(self, pts_path):
    
         #.ptsãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰2ç‚¹ (å·¦ä¸Šã¨å³ä¸‹ãªã©) ã‚’èª­ã¿å–ã‚Šã€
    
        boxes = []
        labels = []

        if not os.path.exists(pts_path):
            return np.empty((0, 4), dtype=np.float32), np.empty((0,), dtype=np.int64)

        xs, ys = [], []
        with open(pts_path, 'r') as f:
            for line in f:
                line = line.strip()
            # ç©ºè¡Œã‚„ãƒ˜ãƒƒãƒ€ãƒ¼ã€æ³¢æ‹¬å¼§ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if not line or line.startswith("version") or line in ["{", "}"]:
                    continue

            # "129 100" ã®ã‚ˆã†ãªåº§æ¨™ãƒšã‚¢ã‚’èª­ã‚€
                parts = line.split()
                if len(parts) != 2:
                    continue

                try:
                    x, y = float(parts[0]), float(parts[1])
                    xs.append(x)
                    ys.append(y)
                except ValueError:
                    continue

        if len(xs) >= 2 and len(ys) >= 2:
            xmin, xmax = min(xs), max(xs)
            ymin, ymax = min(ys), max(ys)
            boxes = np.array([[xmin, ymin, xmax, ymax]], dtype=np.float32)
            labels = np.array([1], dtype=np.int64)  # â† å…¨ã¦åŒã˜ã‚¯ãƒ©ã‚¹æ‰±ã„
        else:
            # ç‚¹ãŒè¶³ã‚Šãªã„å ´åˆã¯ç©ºã«ã—ã¦ãŠã
            boxes = np.empty((0, 4), dtype=np.float32)
            labels = np.empty((0,), dtype=np.int64)

        return boxes, labels

        
    def __getitem__(self, idx):
        # 1. ç”»åƒã¨PTSãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        # self.imgs ã«ã¯ 'dataset/img001.jpg' ã®ã‚ˆã†ãªç›¸å¯¾ãƒ‘ã‚¹ãŒå…¥ã£ã¦ã„ã‚‹ã“ã¨ã‚’æƒ³å®š
        img_path_full = self.imgs[idx]
        
        # rootã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡ºï¼ˆimg_listãŒçµ¶å¯¾ãƒ‘ã‚¹ã®å ´åˆã€ã“ã“ã§ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã ã‘æŠ½å‡ºã™ã‚‹ï¼‰
        img_filename = os.path.basename(img_path_full)
        base_name = os.path.splitext(img_filename)[0]
        pts_filename = base_name + ".pts"
        
        # .ptsãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ä½œæˆ
        pts_path = os.path.join(self.root, pts_filename)

        """"
        # 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        img = Image.open(img_path_full).convert("RGB") # ğŸ‘ˆ ä¿®æ­£: img_path_fullã‚’ä½¿ç”¨
        boxes_np, labels_np = self._parse_pts(pts_path)

        # 3. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¾æ›¸ã®ä½œæˆï¼ˆRetinaNetã®è¦æ±‚å½¢å¼ï¼‰
        if boxes_np.size == 0:
            boxes = torch.empty((0, 4), dtype=torch.float32)
            labels = torch.empty((0,), dtype=torch.int64)
        else:
            boxes = torch.as_tensor(boxes_np, dtype=torch.float32)
            labels = torch.as_tensor(labels_np, dtype=torch.int64)
        
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = torch.tensor([idx])
        
        # 4. å¤‰æ›ï¼ˆtransformsï¼‰ã®é©ç”¨
        if self.transforms is not None:
            img = self.transforms(img)

        return img, target
        """
        # 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        img = Image.open(img_path_full).convert("RGB")
        boxes_np, labels_np = self._parse_pts(pts_path)

        # 3. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¾æ›¸ã®ä½œæˆã¨ v2 å½¢å¼ã¸ã®å¤‰æ› ğŸ‘ˆ ã“ã“ã‚’ä¿®æ­£

        # 3-1. ç”»åƒã®ã‚µã‚¤ã‚ºã‚’å–å¾— (H, W) 224Ã—224
        W, H = img.size # PIL Imageã®ã‚µã‚¤ã‚ºã¯ (W, H)

        if boxes_np.size == 0:
            # BBOXãŒãªã„å ´åˆã¯ç©ºã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
            boxes_tensor = torch.empty((0, 4), dtype=torch.float32)
        else:
            boxes_tensor = torch.as_tensor(boxes_np, dtype=torch.float32)

        labels_tensor = torch.as_tensor(labels_np, dtype=torch.int64)

        # 3-2. v2 å½¢å¼ã® BoundingBoxes ã«å¤‰æ›
        boxes_v2 = BoundingBoxes(
            boxes_tensor, 
            format="XYXY",  # ã‚ãªãŸã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«åˆã‚ã›ã‚‹
            canvas_size=(H, W)
        )
        
        target = {}
        target["boxes"] = boxes_v2 # ğŸ‘ˆ v2å½¢å¼ã®BBOXã‚’æ ¼ç´
        target["labels"] = labels_tensor
        target["image_id"] = torch.tensor([idx])
        
        # 4. å¤‰æ›ï¼ˆtransformsï¼‰ã®é©ç”¨ ğŸ‘ˆ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚‚ä¸€ç·’ã«æ¸¡ã™
        if self.transforms is not None:
            # v2ã§ã¯ã€Transformsã«ç”»åƒã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ä¸¡æ–¹ã‚’æ¸¡ã™
            img, target = self.transforms(img, target) 

        # å¤‰æ›å¾Œã€target["boxes"] ã¯ BoundingBoxes ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¾ã¾ãªã®ã§ã€
        # ãã®ã¾ã¾RetinaNetã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

        return img, target

    def __len__(self):
        return len(self.imgs)

# Transformsã®å®šç¾© ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
"""""
def get_transform(train):
    transforms = []
    transforms.append(T.ToTensor())
    if train:
        transforms.append(T.RandomHorizontalFlip(0.5))
        transforms.append(T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2))
    return T.Compose(transforms)
"""

# Transformsã®å®šç¾© ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’v2ã«ç½®ãæ›ãˆã‚‹
def get_transform(train):
    transforms = []
    # v2ã®ToTensor()ã‚’ä½¿ç”¨: PIL Image/NumPy array -> Tensorã«å¤‰æ›
    transforms.append(T_v2.ToTensor()) 
    
    if train:
        # v2ã®RandomHorizontalFlipã‚’ä½¿ç”¨: BBOXã‚‚è‡ªå‹•ã§ãƒ•ãƒªãƒƒãƒ—ã•ã‚Œã‚‹
        transforms.append(T_v2.RandomHorizontalFlip(0.5))
        # v2ã®ColorJitterã‚’ä½¿ç”¨
        transforms.append(T_v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2))
        
    # T.Composeã§ã¯ãªãT_v2.Composeã‚’ä½¿ç”¨
    return T_v2.Compose(transforms)


# Collate Functionã®å®šç¾©
def custom_collate_fn(batch):
    images = [item[0] for item in batch]
    targets = [item[1] for item in batch]
    return images, targets

# =========================================================
# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åˆ†å‰² (ã“ã®éƒ¨åˆ†ãŒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²ã®æ ¸å¿ƒã§ã™)
# =========================================================

# ãƒ‡ãƒ¼ã‚¿ã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šï¼ˆç”»åƒã¨.ptsãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´æ‰€ï¼‰
DATA_ROOT = '/workspace/dataset'

# 1. å…¨ã¦ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
# os.path.join(DATA_ROOT, "*.jpg") ã¯ã€ä¾‹: /workspace/dataset/*.jpg ã«ãªã‚Šã¾ã™
all_imgs = sorted(glob.glob(os.path.join(DATA_ROOT, "*.jpg")))

print(f"ç™ºè¦‹ã—ãŸå…¨ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(all_imgs)}")

# 2. å­¦ç¿’ç”¨ (80%) ã¨ãƒ†ã‚¹ãƒˆç”¨ (20%) ã«åˆ†å‰²
# test_size=0.2 ã§ 20% ã‚’ãƒ†ã‚¹ãƒˆç”¨ã«å‰²ã‚Šå½“ã¦ã‚‹
train_imgs, test_imgs = train_test_split(
    all_imgs, 
    test_size=0.2, 
    random_state=42 # ã‚·ãƒ¼ãƒ‰å›ºå®šã§å†ç¾æ€§ã‚’ç¢ºä¿
)

print(f"å­¦ç¿’ç”¨ã‚µãƒ³ãƒ—ãƒ«æ•° (80%): {len(train_imgs)}, ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«æ•° (20%): {len(test_imgs)}")


# 3. Datasetã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆï¼ˆåˆ†å‰²ã—ãŸãƒªã‚¹ãƒˆã‚’æ¸¡ã™ï¼‰
train_dataset = CustomObjectDetectionDataset(train_imgs, DATA_ROOT, get_transform(train=True))
test_dataset = CustomObjectDetectionDataset(test_imgs, DATA_ROOT, get_transform(train=False))


# 4. DataLoaderã®ä½œæˆ
train_loader = DataLoader(
    train_dataset,
    batch_size=16, 
    shuffle=True,
    num_workers=2, 
    collate_fn=custom_collate_fn 
)

# âš ï¸ ãƒ†ã‚¹ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼ã‚‚ä½œæˆ
test_loader = DataLoader(
    test_dataset,
    batch_size=16, 
    shuffle=False, # è©•ä¾¡æ™‚ã¯ã‚·ãƒ£ãƒƒãƒ•ãƒ«ä¸è¦
    num_workers=2, 
    collate_fn=custom_collate_fn 
)

# =========================================================
# ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨å­¦ç¿’ãƒ«ãƒ¼ãƒ— (å¤‰æ›´ãªã—)
# =========================================================

# ResNet18ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹
custom_backbone = resnet18(pretrained=False) 

# FPNã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®è¨­å®š
out_channels = 256

backbone_fpn = _resnet_fpn_extractor(
    custom_backbone, 
    trainable_layers=5, 
    extra_blocks=LastLevelP6P7(out_channels, out_channels)
)


# =========================================================
# 1ï¸âƒ£ FPN å‡ºåŠ›å±¤æ•°ã‚’ç¢ºèªã—ã¦ AnchorGenerator ã‚’è‡ªå‹•è¨­å®š
# =========================================================

# ãƒ€ãƒŸãƒ¼ç”»åƒã‚’FPNã«é€šã—ã¦å‡ºåŠ›å±¤ã®æ§‹é€ ã‚’ç¢ºèª
with torch.no_grad():
    dummy_image = torch.rand(1, 3, 224, 224)  # ãƒãƒƒãƒã‚µã‚¤ã‚º1
    features = backbone_fpn(dummy_image)
    print("FPN å‡ºåŠ›å±¤ã®ã‚­ãƒ¼:", list(features.keys()))
    print("å„å±¤ã®å‡ºåŠ›å½¢çŠ¶:")
    for k, v in features.items():
        print(f"  {k}: {tuple(v.shape)}")

num_feature_maps = len(features)
print("FPN å‡ºåŠ›å±¤æ•°:", num_feature_maps)

# AnchorGenerator ã‚’å‡ºåŠ›å±¤æ•°ã«åˆã‚ã›ã¦ä½œæˆ
base_sizes = [8, 16, 32, 64, 128, 224]
sizes_for_anchor = tuple((s,) for s in base_sizes[:num_feature_maps])

anchor_generator = AnchorGenerator(
    sizes=sizes_for_anchor,
    aspect_ratios=((0.5, 1.0, 2.0),) * num_feature_maps
)

print("AnchorGenerator è¨­å®š:", anchor_generator)


# RetinaNetãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
NUM_CLASSES = 2

model = RetinaNet(
    backbone=backbone_fpn,
    num_classes=NUM_CLASSES,
    anchor_generator=anchor_generator
)

# ==========================================================
# å­¦ç¿’ãƒ»è©•ä¾¡ãƒ«ãƒ¼ãƒ—ï¼ˆRetinaNet + IoU/ç²¾åº¦çµ±åˆï¼‰
# ==========================================================

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
optimizer = optim.SGD(
    model.parameters(), 
    lr=0.001,
    momentum=0.9,
    weight_decay=0.001
)

# è©•ä¾¡é–¢æ•°
def evaluate_retinanet(model, dataloader, device, iou_threshold=0.5):
    """
    1ç”»åƒã«ã¤ãäºˆæ¸¬ã‚’1ã¤ã ã‘ã«åˆ¶é™ã—ã¦è©•ä¾¡
    æ­£è§£ãƒœãƒƒã‚¯ã‚¹ã‚‚1ã¤ã ã‘ã®æƒ³å®š
    """
    model.eval()
    
    total_ground_truth_boxes = 0
    total_pred_boxes = 0
    total_correct_detections_for_recall = 0
    total_correct_detections_for_precision = 0
    total_iou_sum = 0.0

    with torch.no_grad():
        for images, targets in tqdm(dataloader, desc="Evaluating"):
            images = [img.to(device).to(torch.float32) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            outputs = model(images)

            for output, target in zip(outputs, targets):
                pred_boxes = output['boxes']
                scores = output['scores']  # ã‚¹ã‚³ã‚¢ã‚‚å–å¾—
                true_boxes = target['boxes']

                # --- äºˆæ¸¬ã‚’1ã¤ã ã‘ã«åˆ¶é™ ---
                if pred_boxes.size(0) > 0:
                    max_idx = scores.argmax()
                    pred_boxes = pred_boxes[max_idx].unsqueeze(0)  # [1,4]

                total_pred_boxes += pred_boxes.size(0)

                if true_boxes.size(0) == 0:
                    continue  # æ­£è§£BOXãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

                total_ground_truth_boxes += true_boxes.size(0)

                if pred_boxes.size(0) == 0:
                    continue  # äºˆæ¸¬BOXãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

                ious = box_iou(pred_boxes, true_boxes)  # [1,1] ã®æƒ³å®š

                # Recall (æ­£è§£BOXåŸºæº–)
                if ious.max() >= iou_threshold:
                    total_correct_detections_for_recall += 1
                    total_iou_sum += ious.max().item()

                # Precision (äºˆæ¸¬BOXåŸºæº–)
                if ious.max() >= iou_threshold:
                    total_correct_detections_for_precision += 1

    # æŒ‡æ¨™è¨ˆç®—
    recall = (total_correct_detections_for_recall / total_ground_truth_boxes * 100.0
              if total_ground_truth_boxes > 0 else 0.0)
    precision = (total_correct_detections_for_precision / total_pred_boxes * 100.0
                 if total_pred_boxes > 0 else 0.0)
    avg_iou = (total_iou_sum / total_correct_detections_for_recall
               if total_correct_detections_for_recall > 0 else 0.0)

    print(f"\n--- è©•ä¾¡çµæœ (1äºˆæ¸¬/ç”»åƒ) ---")
    print(f"Recall (IoU > {iou_threshold}): {recall:.2f}% ({total_correct_detections_for_recall}/{total_ground_truth_boxes})")
    print(f"Precision (IoU > {iou_threshold}): {precision:.2f}% ({total_correct_detections_for_precision}/{total_pred_boxes})")
    print(f"Average IoU: {avg_iou:.4f}")

    return avg_iou, recall, precision

# ==========================================================
# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
# ==========================================================
num_epochs = 20

for epoch in range(num_epochs):
    print(f"\n=== Epoch {epoch+1}/{num_epochs} ===")
    model.train()
    total_epoch_loss = 0.0

    for step, (images, targets) in enumerate(tqdm(train_loader, desc="Training")):
        images = [img.to(device).to(torch.float32) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        optimizer.zero_grad()
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        total_epoch_loss += losses.item()

        # NaNãƒã‚§ãƒƒã‚¯
        if torch.isnan(losses):
            print(f"âš ï¸ NaN detected at step {step}, skipping this batch.")
            continue

        losses.backward()
        optimizer.step()

        # ãƒ­ã‚°
        if step % 50 == 0:
            print(f"Step {step}, Total Loss: {losses.item():.4f}, "
                  f"Cls Loss: {loss_dict['classification'].item():.4f}, "
                  f"Box Loss: {loss_dict['bbox_regression'].item():.4f}")

    print(f"--- Epoch {epoch+1} å®Œäº†: å¹³å‡æå¤± {total_epoch_loss/len(train_loader):.4f} ---")

    # 10ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«è©•ä¾¡
    if (epoch + 1) % 5 == 0:
        print(f"\n--- è©•ä¾¡ (Epoch {epoch+1}) ---")
        evaluate_retinanet(model, test_loader, device, iou_threshold=0.5)
        torch.save(model.state_dict(), f"retinanet_epoch{epoch+1}.pth")
        print(f"Checkpoint saved: retinanet_epoch{epoch+1}.pth")

# ==========================================================
# å­¦ç¿’å®Œäº†å¾Œã®æœ€çµ‚è©•ä¾¡
# ==========================================================
torch.save(model.state_dict(), 'retinanet_custom_weights_final.pth')
print("\n--- æœ€çµ‚è©•ä¾¡ ---")
evaluate_retinanet(model, test_loader, device, iou_threshold=0.5)