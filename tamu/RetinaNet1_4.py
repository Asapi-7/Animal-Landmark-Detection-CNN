import os
import torch
import numpy as np
import time
from PIL import Image
from torch.utils.data import Dataset
from torchvision.transforms import functional as F
from torchvision import transforms as T
from torch.utils.data import DataLoader
import glob # ğŸ‘ˆ è¿½åŠ : ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆå–å¾—ç”¨
from sklearn.model_selection import train_test_split # ğŸ‘ˆ è¿½åŠ : ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ç”¨

# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ç”¨
from resnet18_backbone import resnet18
from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor
from torchvision.ops.feature_pyramid_network import LastLevelP6P7 
from torchvision.models.detection.anchor_utils import AnchorGenerator
from torchvision.models.detection import RetinaNet
from tqdm import tqdm

import torch.optim as optim
from torchvision.ops import box_iou

import random
import cv2

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
class CustomObjectDetectionDataset(Dataset):
    def __init__(self, img_list, root, transforms=None, augment=False):
        """
        img_list: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        root: .ptsãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚ã‚‹ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        augment: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è¡Œã†ã‹ã©ã†ã‹
        """
        self.root = root
        self.imgs = img_list
        self.transforms = transforms
        self.augment = augment

        # ã‚«ãƒ©ãƒ¼ã‚¸ãƒƒã‚¿ãƒ¼è¨­å®šï¼ˆBBoxã«å½±éŸ¿ã—ãªã„ï¼‰
        self.color_transform = T.ColorJitter(
            brightness=0.2, contrast=0.2, saturation=0.2
        )
        self.to_tensor = T.ToTensor()

    def _parse_pts(self, pts_path):
        boxes = []
        labels = []

        if not os.path.exists(pts_path):
            return np.empty((0, 4), dtype=np.float32), np.empty((0,), dtype=np.int64)

        xs, ys = [], []
        with open(pts_path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("version") or line in ["{", "}"]:
                    continue
                parts = line.split()
                if len(parts) != 2:
                    continue
                try:
                    x, y = float(parts[0]), float(parts[1])
                    xs.append(x)
                    ys.append(y)
                except ValueError:
                    continue

        if len(xs) >= 2 and len(ys) >= 2:
            xmin, xmax = min(xs), max(xs)
            ymin, ymax = min(ys), max(ys)
            boxes = np.array([[xmin, ymin, xmax, ymax]], dtype=np.float32)
            labels = np.array([1], dtype=np.int64)
        else:
            boxes = np.empty((0, 4), dtype=np.float32)
            labels = np.empty((0,), dtype=np.int64)

        return boxes, labels

    def __getitem__(self, idx):
        img_path_full = self.imgs[idx]
        img_filename = os.path.basename(img_path_full)
        base_name = os.path.splitext(img_filename)[0]
        pts_path = os.path.join(self.root, base_name + ".pts")

        # --- ç”»åƒèª­ã¿è¾¼ã¿ (OpenCVã§BGRâ†’RGBå¤‰æ›)
        img = cv2.imread(img_path_full)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        H, W, _ = img.shape

        # --- BBoxå–å¾—
        boxes_np, labels_np = self._parse_pts(pts_path)

        # --- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ ---
        if self.augment and boxes_np.size > 0:
            x1, y1, x2, y2 = boxes_np[0]

            # 1. å·¦å³åè»¢ï¼ˆç¢ºç‡50%ï¼‰
            if random.random() > 0.5:
                img = cv2.flip(img, 1)
                x1_new = W - x2
                x2_new = W - x1
                x1, x2 = x1_new, x2_new

            boxes_np = np.array([[x1, y1, x2, y2]], dtype=np.float32)

            # 2. è‰²å¤‰æ›ï¼ˆBBoxã«å½±éŸ¿ã—ãªã„ï¼‰
            pil_img = T.functional.to_pil_image(img)
            img = self.color_transform(pil_img)
            img = T.functional.to_tensor(img)  # PILâ†’Tensor
        else:
            img = self.to_tensor(img)

        # --- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ ---
        if boxes_np.size == 0:
            boxes = torch.empty((0, 4), dtype=torch.float32)
            labels = torch.empty((0,), dtype=torch.int64)
        else:
            boxes = torch.as_tensor(boxes_np, dtype=torch.float32)
            labels = torch.as_tensor(labels_np, dtype=torch.int64)

        target = {
            "boxes": boxes,
            "labels": labels,
            "image_id": torch.tensor([idx])
        }

        return img, target

    def __len__(self):
        return len(self.imgs)

# ç”»åƒå¤‰æ›ï¼ˆãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ ï¼‰ã‚’è¿”ã™é–¢æ•°
def get_transform(train=False):
    transforms = []

    # PIL â†’ Tensor å¤‰æ›
    transforms.append(T.ToTensor())

    return T.Compose(transforms)


# Collate Functionã®å®šç¾©
def custom_collate_fn(batch):
    images = [item[0] for item in batch]
    targets = [item[1] for item in batch]
    return images, targets

# =========================================================
# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åˆ†å‰² (ã“ã®éƒ¨åˆ†ãŒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²ã®æ ¸å¿ƒã§ã™)
# =========================================================

# ãƒ‡ãƒ¼ã‚¿ã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šï¼ˆç”»åƒã¨.ptsãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´æ‰€ï¼‰
DATA_ROOT = '/workspace/dataset'

# 1. å…¨ã¦ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
# os.path.join(DATA_ROOT, "*.jpg") ã¯ã€ä¾‹: /workspace/dataset/*.jpg ã«ãªã‚Šã¾ã™
all_imgs = sorted(glob.glob(os.path.join(DATA_ROOT, "*.jpg")))

print(f"ç™ºè¦‹ã—ãŸå…¨ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(all_imgs)}")

# 2. å­¦ç¿’ç”¨ (80%) ã¨ãƒ†ã‚¹ãƒˆç”¨ (20%) ã«åˆ†å‰²
# test_size=0.2 ã§ 20% ã‚’ãƒ†ã‚¹ãƒˆç”¨ã«å‰²ã‚Šå½“ã¦ã‚‹
train_imgs, test_imgs = train_test_split(
    all_imgs, 
    test_size=0.2, 
    random_state=42 # ã‚·ãƒ¼ãƒ‰å›ºå®šã§å†ç¾æ€§ã‚’ç¢ºä¿
)

print(f"å­¦ç¿’ç”¨ã‚µãƒ³ãƒ—ãƒ«æ•° (80%): {len(train_imgs)}, ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«æ•° (20%): {len(test_imgs)}")


# 3. Datasetã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆï¼ˆåˆ†å‰²ã—ãŸãƒªã‚¹ãƒˆã‚’æ¸¡ã™ï¼‰
train_dataset = CustomObjectDetectionDataset(train_imgs, DATA_ROOT, get_transform(train=True))
test_dataset = CustomObjectDetectionDataset(test_imgs, DATA_ROOT, get_transform(train=False))


# 4. DataLoaderã®ä½œæˆ
train_loader = DataLoader(
    train_dataset,
    batch_size=16, 
    shuffle=True,
    num_workers=2, 
    collate_fn=custom_collate_fn 
)

# âš ï¸ ãƒ†ã‚¹ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼ã‚‚ä½œæˆ
test_loader = DataLoader(
    test_dataset,
    batch_size=16, 
    shuffle=False, # è©•ä¾¡æ™‚ã¯ã‚·ãƒ£ãƒƒãƒ•ãƒ«ä¸è¦
    num_workers=2, 
    collate_fn=custom_collate_fn 
)

# =========================================================
# ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨å­¦ç¿’ãƒ«ãƒ¼ãƒ— (å¤‰æ›´ãªã—)
# =========================================================

# ResNet18ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹
custom_backbone = resnet18(pretrained=False) 

# FPNã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®è¨­å®š
out_channels = 256

backbone_fpn = _resnet_fpn_extractor(
    custom_backbone, 
    trainable_layers=5, 
    extra_blocks=LastLevelP6P7(out_channels, out_channels)
)


# =========================================================
# 1ï¸âƒ£ FPN å‡ºåŠ›å±¤æ•°ã‚’ç¢ºèªã—ã¦ AnchorGenerator ã‚’è‡ªå‹•è¨­å®š
# =========================================================

# ãƒ€ãƒŸãƒ¼ç”»åƒã‚’FPNã«é€šã—ã¦å‡ºåŠ›å±¤ã®æ§‹é€ ã‚’ç¢ºèª
with torch.no_grad():
    dummy_image = torch.rand(1, 3, 224, 224)  # ãƒãƒƒãƒã‚µã‚¤ã‚º1 RGBã®3
    features = backbone_fpn(dummy_image)
    print("FPN å‡ºåŠ›å±¤ã®ã‚­ãƒ¼:", list(features.keys()))
    print("å„å±¤ã®å‡ºåŠ›å½¢çŠ¶:")
    for k, v in features.items():
        print(f"  {k}: {tuple(v.shape)}")

num_feature_maps = len(features)
print("FPN å‡ºåŠ›å±¤æ•°:", num_feature_maps)

# AnchorGenerator ã‚’å‡ºåŠ›å±¤æ•°ã«åˆã‚ã›ã¦ä½œæˆ
base_sizes = [8, 16, 32, 64, 128, 224]
sizes_for_anchor = tuple((s,) for s in base_sizes[:num_feature_maps])

anchor_generator = AnchorGenerator(
    sizes=sizes_for_anchor,
    aspect_ratios=((0.5, 1.0, 2.0),) * num_feature_maps
)

print("AnchorGenerator è¨­å®š:", anchor_generator)


# RetinaNetãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
NUM_CLASSES = 2

model = RetinaNet(
    backbone=backbone_fpn,
    num_classes=NUM_CLASSES,
    anchor_generator=anchor_generator
)

# ==========================================================
# å­¦ç¿’ãƒ»è©•ä¾¡ãƒ«ãƒ¼ãƒ—ï¼ˆRetinaNet + IoU/ç²¾åº¦çµ±åˆï¼‰
# ==========================================================

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
optimizer = optim.SGD(
    model.parameters(), 
    lr=0.001,
    momentum=0.9,
    weight_decay=0.001
)

# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer,
    step_size=5,
    gamma=0.1
)

# è©•ä¾¡é–¢æ•°
def evaluate_retinanet(model, dataloader, device, iou_threshold=0.5):
    """
    1ç”»åƒã«ã¤ãäºˆæ¸¬ã‚’1ã¤ã ã‘ã«åˆ¶é™ã—ã¦è©•ä¾¡
    æ­£è§£ãƒœãƒƒã‚¯ã‚¹ã‚‚1ã¤ã ã‘ã®æƒ³å®š
    """
    model.eval()
    
    total_ground_truth_boxes = 0
    total_pred_boxes = 0
    total_correct_detections_for_recall = 0
    total_correct_detections_for_precision = 0
    total_iou_sum = 0.0

    with torch.no_grad():
        for images, targets in tqdm(dataloader, desc="Evaluating"):
            images = [img.to(device).to(torch.float32) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            outputs = model(images)

            for output, target in zip(outputs, targets):
                pred_boxes = output['boxes']
                scores = output['scores']  # ã‚¹ã‚³ã‚¢ã‚‚å–å¾—
                true_boxes = target['boxes']

                # --- äºˆæ¸¬ã‚’1ã¤ã ã‘ã«åˆ¶é™ ---
                if pred_boxes.size(0) > 0:
                    max_idx = scores.argmax()
                    pred_boxes = pred_boxes[max_idx].unsqueeze(0)  # [1,4]

                total_pred_boxes += pred_boxes.size(0)

                if true_boxes.size(0) == 0:
                    continue  # æ­£è§£BOXãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

                total_ground_truth_boxes += true_boxes.size(0)

                if pred_boxes.size(0) == 0:
                    continue  # äºˆæ¸¬BOXãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

                ious = box_iou(pred_boxes, true_boxes)  # [1,1] ã®æƒ³å®š

                # Recall (æ­£è§£BOXåŸºæº–)
                if ious.max() >= iou_threshold:
                    total_correct_detections_for_recall += 1
                    total_iou_sum += ious.max().item()

                # Precision (äºˆæ¸¬BOXåŸºæº–)
                if ious.max() >= iou_threshold:
                    total_correct_detections_for_precision += 1

    # æŒ‡æ¨™è¨ˆç®—
    recall = (total_correct_detections_for_recall / total_ground_truth_boxes * 100.0
              if total_ground_truth_boxes > 0 else 0.0)
    precision = (total_correct_detections_for_precision / total_pred_boxes * 100.0
                 if total_pred_boxes > 0 else 0.0)
    avg_iou = (total_iou_sum / total_correct_detections_for_recall
               if total_correct_detections_for_recall > 0 else 0.0)

    print(f"\n--- è©•ä¾¡çµæœ (1äºˆæ¸¬/ç”»åƒ) ---")
    print(f"Recall (IoU > {iou_threshold}): {recall:.2f}% ({total_correct_detections_for_recall}/{total_ground_truth_boxes})")
    print(f"Precision (IoU > {iou_threshold}): {precision:.2f}% ({total_correct_detections_for_precision}/{total_pred_boxes})")
    print(f"Average IoU: {avg_iou:.4f}")

    return avg_iou, recall, precision

# ==========================================================
# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
# ==========================================================
num_epochs = 20

for epoch in range(num_epochs):
    print(f"\n=== Epoch {epoch+1}/{num_epochs} ===")
    model.train()
    total_epoch_loss = 0.0

    for step, (images, targets) in enumerate(tqdm(train_loader, desc="Training")):
        images = [img.to(device).to(torch.float32) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        optimizer.zero_grad()
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        total_epoch_loss += losses.item()

        # NaNãƒã‚§ãƒƒã‚¯
        if torch.isnan(losses):
            print(f"âš ï¸ NaN detected at step {step}, skipping this batch.")
            continue

        losses.backward()
        optimizer.step()

        # ãƒ­ã‚°
        if step % 50 == 0:
            print(f"Step {step}, Total Loss: {losses.item():.4f}, "
                  f"Cls Loss: {loss_dict['classification'].item():.4f}, "
                  f"Box Loss: {loss_dict['bbox_regression'].item():.4f}")
            
    # å­¦ç¿’ç‡ã®å‡ºåŠ›
    current_lr = optimizer.param_groups[0]["lr"]
    tqdm.write(f"LR: {current_lr:.6f}")
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—ï¼šå­¦ç¿’ç‡ã‚’èª¿æ•´
    scheduler.step()

    print(f"--- Epoch {epoch+1} å®Œäº†: å¹³å‡æå¤± {total_epoch_loss/len(train_loader):.4f} ---")

    # 10ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«è©•ä¾¡
    if (epoch + 1) % 5 == 0:
        print(f"\n--- è©•ä¾¡ (Epoch {epoch+1}) ---")
        evaluate_retinanet(model, test_loader, device, iou_threshold=0.5)
        torch.save(model.state_dict(), f"retinanet_epoch{epoch+1}.pth")
        print(f"Checkpoint saved: retinanet_epoch{epoch+1}.pth")

# ==========================================================
# å­¦ç¿’å®Œäº†å¾Œã®æœ€çµ‚è©•ä¾¡
# ==========================================================
torch.save(model.state_dict(), 'retinanet_custom_weights_final.pth')
print("\n--- æœ€çµ‚è©•ä¾¡ ---")
evaluate_retinanet(model, test_loader, device, iou_threshold=0.5)